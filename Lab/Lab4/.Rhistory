d_b1 <- d_z1
# Update weights and biases with dropout
dropout_mask <- matrix(runif(hidden_size) > dropout_rate, nrow = hidden_size)
W2 <- W2 - gamma * d_W2 * dropout_mask
b2 <- b2 - gamma * d_b2
W1 <- W1 - gamma * d_W1
b1 <- b1 - gamma * d_b1
}
#===============================
#START
#===============================
# KEYWORD: NN
#===============================
#(1)
#===============================
# Produce the training data in dat
x <- runif(500, -4, 4)
y <- sin(x)
dat <- cbind(x, y)
plot(dat, main = "Training Data")
# Define hyperparameters
gamma <- 0.01  # Learning rate
dropout_rates <- c(0, 0.01, 0.05)  # Different dropout rates to test
# Define activation function (sigmoid) and its derivative
h <- function(z) {
return(1 / (1 + exp(-z)))
}
hprime <- function(z) {
return(h(z) * (1 - h(z)))
}
# Function to compute predictions
yhat <- function(x, W1, b1, W2, b2) {
z1 <- x %*% W1 + b1
q1 <- h(z1)  # Hidden layer activation
z2 <- q1 %*% W2 + b2
q2 <- z2  # Output layer (identity function, as it's regression)
return(q2)
}
# Function to compute Mean Squared Error (MSE)
MSE <- function(dat, W1, b1, W2, b2) {
predictions <- apply(dat[,1, drop = FALSE], 1, yhat, W1, b1, W2, b2)
return(mean((predictions - dat[,2])^2))
}
# Run training with different dropout rates
for (dropout_rate in dropout_rates) {
# Initialize parameters
set.seed(42)  # For reproducibility
input_size <- 1  # One input feature
hidden_size <- 2  # Two hidden units
output_size <- 1  # One output unit
# Randomly initialize weights and biases
W1 <- matrix(runif(input_size * hidden_size, -0.5, 0.5), nrow = input_size)
b1 <- runif(hidden_size, -0.5, 0.5)
W2 <- matrix(runif(hidden_size * output_size, -0.5, 0.5), nrow = hidden_size)
b2 <- runif(output_size, -0.5, 0.5)
# Initialize results container
res <- c()
# Training loop
num_iterations <- 100000
for (i in 1:num_iterations) {
if (i %% 1000 == 0) {
res <- c(res, MSE(dat, W1, b1, W2, b2))
}
# Randomly select a training example
j <- sample(1:nrow(dat), 1)
x_j <- dat[j, 1, drop = FALSE]
y_j <- dat[j, 2]
# Forward propagation
z1 <- x_j %*% W1 + b1
q1 <- h(z1)
z2 <- q1 %*% W2 + b2
q2 <- z2
# Compute the error
error <- q2 - y_j
# Backward propagation
d_q2 <- error  # Derivative of loss w.r.t. output
d_z2 <- d_q2
d_W2 <- t(q1) %*% d_z2
d_b2 <- d_z2
d_q1 <- d_z2 %*% t(W2) * hprime(z1)
d_z1 <- d_q1
d_W1 <- t(x_j) %*% d_z1
d_b1 <- d_z1
# Dropout mask for the hidden layer
dropout_mask <- matrix(runif(hidden_size) > dropout_rate, nrow = hidden_size)
# Update weights and biases
W2 <- W2 - gamma * d_W2 * dropout_mask
b2 <- b2 - gamma * d_b2
W1 <- W1 - gamma * d_W1
b1 <- b1 - gamma * d_b1
}
# Plot MSE over iterations for current dropout rate
plot(res, type = "l", main = paste("MSE with Dropout Rate", dropout_rate))
# Plot training data and predictions
plot(dat, main = paste("Training Data and Predictions (Dropout Rate", dropout_rate, ")"))
points(dat[,1], apply(dat[,1, drop = FALSE], 1, yhat, W1, b1, W2, b2), col = 'red')
}
#===============================
#START
#===============================
# KEYWORD: NN
#===============================
#(1)
#===============================
# Generate the training data
x <- runif(500, -4, 4)
y <- sin(x)
dat <- cbind(x, y)
plot(dat, main = "Training Data")
# Define hyperparameters and functions
gamma <- 0.01  # Learning rate
dropout_rates <- c(0, 0.01, 0.05)  # Different dropout rates to test
# Define the activation function (sigmoid) and its derivative
h <- function(z) {
return(1 / (1 + exp(-z)))
}
hprime <- function(z) {
return(h(z) * (1 - h(z)))
}
# Function to compute predictions
yhat <- function(x, W1, b1, W2, b2) {
z1 <- x %*% W1 + b1
q1 <- h(z1)  # Hidden layer activation
z2 <- q1 %*% W2 + b2
q2 <- z2  # Output layer (identity function, as it's regression)
return(q2)
}
# Function to compute Mean Squared Error (MSE)
MSE <- function(dat, W1, b1, W2, b2) {
predictions <- apply(dat[,1, drop = FALSE], 1, yhat, W1, b1, W2, b2)
return(mean((predictions - dat[,2])^2))
}
# Function to train the neural network with dropout
train_neural_network <- function(dropout_rate, num_iterations = 100000, gamma = 0.01) {
# Initialize parameters
set.seed(42)  # For reproducibility
input_size <- 1  # One input feature
hidden_size <- 2  # Two hidden units
output_size <- 1  # One output unit
# Randomly initialize weights and biases
W1 <- matrix(runif(input_size * hidden_size, -0.5, 0.5), nrow = input_size)
b1 <- runif(hidden_size, -0.5, 0.5)
W2 <- matrix(runif(hidden_size * output_size, -0.5, 0.5), nrow = hidden_size)
b2 <- runif(output_size, -0.5, 0.5)
# Initialize results container
res <- c()
# Training loop
for (i in 1:num_iterations) {
if (i %% 1000 == 0) {
res <- c(res, MSE(dat, W1, b1, W2, b2))
}
# Randomly select a training example
j <- sample(1:nrow(dat), 1)
x_j <- dat[j, 1, drop = FALSE]
y_j <- dat[j, 2]
# Forward propagation
z1 <- x_j %*% W1 + b1
q1 <- h(z1)
z2 <- q1 %*% W2 + b2
q2 <- z2
# Compute the error
error <- q2 - y_j
# Backward propagation
d_q2 <- error  # Derivative of loss w.r.t. output
d_z2 <- d_q2
d_W2 <- t(q1) %*% d_z2
d_b2 <- d_z2
d_q1 <- d_z2 %*% t(W2) * hprime(z1)
d_z1 <- d_q1
d_W1 <- t(x_j) %*% d_z1
d_b1 <- d_z1
# Dropout mask for the hidden layer
dropout_mask <- matrix(runif(hidden_size) > dropout_rate, nrow = hidden_size)
# Update weights and biases
W2 <- W2 - gamma * d_W2 * dropout_mask
b2 <- b2 - gamma * d_b2
W1 <- W1 - gamma * d_W1
b1 <- b1 - gamma * d_b1
}
# Return results
return(list(MSE = res, W1 = W1, b1 = b1, W2 = W2, b2 = b2))
}
# Function to plot results
plot_results <- function(res, dropout_rate, W1, b1, W2, b2) {
# Plot MSE over iterations
plot(res, type = "l", main = paste("MSE over Iterations (Dropout Rate:", dropout_rate, ")"),
xlab = "Iterations", ylab = "MSE")
# Plot training data and predictions
predictions <- apply(dat[,1, drop = FALSE], 1, yhat, W1, b1, W2, b2)
plot(dat, main = paste("Training Data and Predictions (Dropout Rate:", dropout_rate, ")"))
points(dat[,1], predictions, col = 'red')
}
rate=dropout_rates[1]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
rate=dropout_rates[2]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
rate=dropout_rates[3]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
t
# Function to plot results
plot_results <- function(res, dropout_rate, W1, b1, W2, b2) {
# Set up the plotting area to have two plots side by side
par(mfrow = c(1, 2))
# Plot MSE over iterations
plot(res, type = "l", main = paste("MSE over Iterations (Dropout Rate:", dropout_rate, ")"),
xlab = "Iterations", ylab = "MSE")
# Plot training data and predictions
predictions <- apply(dat[,1, drop = FALSE], 1, yhat, W1, b1, W2, b2)
plot(dat, main = paste("Training Data and Predictions (Dropout Rate:", dropout_rate, ")"))
points(dat[,1], predictions, col = 'red')
# Reset plotting area
par(mfrow = c(1, 1))
}
rate=dropout_rates[1]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
rate=dropout_rates[2]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
rate=dropout_rates[3]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
#===============================
# (3) K-means algorithm
#===============================
# Load necessary library
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Parameters for the mixture model
n <- 300 # total number of samples
mu1 <- c(2, 2)
mu2 <- c(-3, -1)
mu3 <- c(5, -4)
sigma1 <- matrix(c(1, 0.5, 0.5, 1), ncol=2)
sigma2 <- matrix(c(1.5, -0.5, -0.5, 1.5), ncol=2)
sigma3 <- matrix(c(1, 0, 0, 1), ncol=2)
# Generate samples from each Gaussian distribution
data1 <- mvrnorm(n/3, mu1, sigma1)
data2 <- mvrnorm(n/3, mu2, sigma2)
data3 <- mvrnorm(n/3, mu3, sigma3)
# Combine the data
data <- rbind(data1, data2, data3)
# Convert data to a data frame
data_df <- data.frame(x = data[, 1], y = data[, 2])
# Plot the generated data
ggplot(data_df, aes(x = x, y = y)) +
geom_point(alpha = 0.5) +
ggtitle("Sampled Data from Mixture Model") +
theme_minimal()
#===============================
# (3) K-means algorithm
#===============================
# Load necessary library
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Parameters for the mixture model
n <- 300 # total number of samples
mu1 <- c(2, 2)
mu2 <- c(-3, -1)
mu3 <- c(5, -4)
sigma1 <- matrix(c(1, 0.5, 0.5, 1), ncol=2)
sigma2 <- matrix(c(1.5, -0.5, -0.5, 1.5), ncol=2)
sigma3 <- matrix(c(1, 0, 0, 1), ncol=2)
# Generate samples from each Gaussian distribution
data1 <- mvrnorm(n/3, mu1, sigma1)
#===============================
# (3) K-means algorithm
#===============================
# Load necessary library
library(MASS)  # For mvrnorm function
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Parameters for the mixture model
n <- 300 # total number of samples
mu1 <- c(2, 2)
mu2 <- c(-3, -1)
mu3 <- c(5, -4)
sigma1 <- matrix(c(1, 0.5, 0.5, 1), ncol=2)
sigma2 <- matrix(c(1.5, -0.5, -0.5, 1.5), ncol=2)
sigma3 <- matrix(c(1, 0, 0, 1), ncol=2)
# Generate samples from each Gaussian distribution
data1 <- mvrnorm(n/3, mu1, sigma1)
data2 <- mvrnorm(n/3, mu2, sigma2)
data3 <- mvrnorm(n/3, mu3, sigma3)
# Combine the data
data <- rbind(data1, data2, data3)
# Convert data to a data frame
data_df <- data.frame(x = data[, 1], y = data[, 2])
# Plot the generated data
ggplot(data_df, aes(x = x, y = y)) +
geom_point(alpha = 0.5) +
ggtitle("Sampled Data from Mixture Model") +
theme_minimal()
# Set number of clusters
k <- 3
# Run K-means algorithm
kmeans_result <- kmeans(data_df, centers = k, nstart = 25)
# Add cluster assignment to the data frame
data_df$cluster <- as.factor(kmeans_result$cluster)
# Plot the clusters
ggplot(data_df, aes(x = x, y = y, color = cluster)) +
geom_point(alpha = 0.5) +
ggtitle("K-means Clustering Results") +
theme_minimal()
# Print the cluster centers
print(kmeans_result$centers)
# Print the cluster assignments
table(data_df$cluster)
#===============================
#(2) SVM ksvm
#===============================
# Load necessary libraries
library(kernlab)
library(caret)
# Load the spam dataset from the kernlab package
data(spam)
# Define the training and testing datasets
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(spam$type, p = .8,
list = FALSE,
times = 1)
spamTrain <- spam[ trainIndex,]
spamTest  <- spam[-trainIndex,]
# Define the C values to be tested
C_values <- c(0.5, 1, 5)
kernel_width <- 0.05
# Prepare to store results
results <- data.frame(C = numeric(), Accuracy = numeric())
# Model training and evaluation
for (C in C_values) {
# Train the SVM model
model <- ksvm(type ~ ., data = spamTrain, kernel = "rbfdot", kpar = list(sigma = kernel_width), C = C)
# Make predictions on the test set
predictions <- predict(model, spamTest)
# Calculate accuracy
accuracy <- sum(predictions == spamTest$type) / nrow(spamTest)
# Store the results
results <- rbind(results, data.frame(C = C, Accuracy = accuracy))
}
# Display the results
print(results)
# Select the best model based on accuracy
best_model_params <- results[which.max(results$Accuracy), ]
best_C <- best_model_params$C
cat("The best C value is:", best_C, "with accuracy:", best_model_params$Accuracy, "\n")
# Train the best model on the entire training data
final_model <- ksvm(type ~ ., data = spam, kernel = "rbfdot", kpar = list(sigma = kernel_width), C = best_C)
# Save the final model (optional)
save(final_model, file = "final_svm_model.RData")
# Print the code of the final model
print(final_model)
#===============================
#START 2023-8-2
#===============================
# KEYWORD:SVM KMEANS
#===============================
#(1) SVM
#===============================
library(kernlab)
# Create training data (x1, x2: predictors, x3: target)
x1 <- sample(0:1,1000,replace = TRUE)
x2 <- sample(0:1,1000,replace = TRUE)
x3 <- as.numeric(xor(x1,x2))
foo <- runif(1000,min = -0.2,max = 0.2)
x1 <- x1 + foo
foo <- runif(1000,min = -0.2,max = 0.2)
x2 <- x2 + foo
# Visualize training data.
plot(cbind(x1,x2),type = "n")
text(cbind(x1,x2),labels = x3)
# Learn SVM with RBF kernel and check training error.
foo <- ksvm(cbind(x1,x2), x3, kernel = "rbfdot", type = 'C-svc')
foo
# Visualize predictions for training data.
prex3 <- predict(foo, cbind(x1,x2))
plot(cbind(x1,x2), type = "n")
text(cbind(x1,x2), labels = prex3)
#===============================
#START 2023-8-2
#===============================
# KEYWORD:SVM KMEANS
#===============================
#(1) SVM
#===============================
#The code below trains a support vector machine (SVM) for classification. The problem consists of two
#continuous inputs, one binary target and 1000 training points. Thus, the problem may seem rather easy.
#However, the SVM does not perform great. Explain why.
library(kernlab)
# Create training data (x1, x2: predictors, x3: target)
x1 <- sample(0:1,1000,replace = TRUE)
x2 <- sample(0:1,1000,replace = TRUE)
x3 <- as.numeric(xor(x1,x2))
foo <- runif(1000,min = -0.2,max = 0.2)
x1 <- x1 + foo
foo <- runif(1000,min = -0.2,max = 0.2)
x2 <- x2 + foo
# Visualize training data.
plot(cbind(x1,x2),type = "n")
text(cbind(x1,x2),labels = x3)
# Learn SVM and check training error.
foo <- ksvm(cbind(x1,x2),x3,kernel = "vanilladot",type = 'C-svc')
foo
# Visualize predictions for training data.
prex3 <- predict(foo,cbind(x1,x2))
plot(cbind(x1,x2),type = "n")
text(cbind(x1,x2),labels = prex3)
library(plotly)
###########################  Code For Assignment 2.8 ###########################
library(shiny)
install.packages("TTR")
library(TTR)
djia = getYahooData("^DJI", start=20060420, end=20160420, freq="daily")
install.packages("quantmod")
install.packages("bnlearn")
library(bnlearn)
data("asia")
bns = list()
for (i in 1:2) {
bns[[i]] = hc(asia, random.graph(colnames(asia)))
plot(bns[[i]])
}
i=1
bns[[i]] = hc(asia, random.graph(colnames(asia)))
plot(bns[[i]])
i=2
bns[[i]] = hc(asia, random.graph(colnames(asia)))
plot(bns[[i]])
all.equal(vstructs(bns[[1]]), vstructs(bns[[2]]))
install.packages("gRain")
library(bnlearn)
library(gRain)
data("asia")
force(asia)
View(asia)
license()
setwd("~/Desktop/GIT/Visualization/Lab/Lab2")
setwd("~/Desktop/GIT/Visualization/Lab/Lab4")
library(plotly)
df=lattice::barley
p<-ggplot(df, aes(y=variety, x=yield, color=year))+geom_point()+
facet_grid(site~.)
p
ggplotly(p)
df1<-mpg
p1<-ggplot(df1, aes(cty, displ))+geom_point()+geom_smooth()+
facet_wrap(~class, labeller = "label_both")
p1
data2 <- read.csv("data/adult.csv", header = FALSE)
colnames(data2) <- c("age", "workclass", "fnlwgt", "education", "education_num",
"marital_status", "occupation","relationship", "race", "sex",
"capital_gain", "capital_loss", "hours_per_week", "native_country",
"income_level")
data2$income_level <- as.factor(data2$income_level)
data2_2.3 <- data2 %>% mutate(age_group = cut_number(age, 6))
View(data2_2.3)
trellis_plot_2.3 <- ggplot(data2_2.3, aes(x = education_num, y = capital_loss)) +
geom_point()
trellis_plot_2.3
trellis_plot_2.3 <- ggplot(data2_2.3, aes(x = education_num, y = capital_loss)) +
geom_point() +
facet_wrap(~marital_status)
trellis_plot_2.3
trellis_plot_2.3 <- ggplot(data2_2.3, aes(x = education_num, y = capital_loss)) +
geom_point() +
facet_wrap(~age_group)
trellis_plot_2.3
trellis_plot_2.3 <- ggplot(data2_2.3, aes(x = education_num, y = capital_loss)) +
geom_density_2d()
trellis_plot_2.3
trellis_plot_2.3 <- ggplot(data = data2_filtered_2.3, aes(x = education_num, y = capital_loss))
data2_filtered_2.3 <- data2 %>% filter(capital_loss != 0)
data2_filtered_2.3 <- data2_filtered_2.3 %>% mutate(age_group = cut_number(age, 6))
data2_filtered_2.3
trellis_plot_2.3 <- ggplot(data = data2_filtered_2.3, aes(x = education_num, y = capital_loss)) +
geom_density_2d(aes(colour = age_group))
trellis_plot_2.3
trellis_plot_2.3 <- ggplot(data = data2_filtered_2.3, aes(x = education_num, y = capital_loss)) +
geom_density_2d(aes(color = age_group))
trellis_plot_2.3
trellis_plot_2.3 <- ggplot(data = data2_filtered_2.3, aes(x = education_num, y = capital_loss)) +
geom_density_2d(aes(color = age_group)) +
facet_wrap(~age_group)
trellis_plot_2.3
trellis_plot_2.3 <- ggplot(data = data2_filtered_2.3, aes(x = education_num, y = capital_loss)) +
geom_density_2d() +
facet_wrap(~age_group)
trellis_plot_2.3
data2_2.4 <- data2 %>% mutate(age_group_cutnumber = cut_number(age, 4)) %>%
#  Shingles with 10% overlap
mutate(age_group_shingles = cut_interval(age, n = 4, overlap = 0.1))
data2_2.4
cut_interval(data2$age, n = 4, overlap = 0.1)
lattice::equal.count(data2$age, number=4, overlap=0.1)
data2$age
ages = lattice::equal.count(data2$age, number=4, overlap=0.1)
levels(ages)
data2_2.4 <- data2 %>% mutate(age_group_cutnumber = cut_number(age, 4)) %>%
#  Shingles with 10% overlap
mutate(age_group_shingles = lattice::equal.count(age, number=4, overlap=0.1))
data2_2.4
