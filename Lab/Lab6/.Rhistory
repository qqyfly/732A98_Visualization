# Plot MSE over iterations
plot(res, type = "l", main = paste("MSE over Iterations (Dropout Rate:", dropout_rate, ")"),
xlab = "Iterations", ylab = "MSE")
# Plot training data and predictions
predictions <- apply(dat[,1, drop = FALSE], 1, yhat, W1, b1, W2, b2)
plot(dat, main = paste("Training Data and Predictions (Dropout Rate:", dropout_rate, ")"))
points(dat[,1], predictions, col = 'red')
}
rate=dropout_rates[1]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
rate=dropout_rates[2]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
rate=dropout_rates[3]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
t
# Function to plot results
plot_results <- function(res, dropout_rate, W1, b1, W2, b2) {
# Set up the plotting area to have two plots side by side
par(mfrow = c(1, 2))
# Plot MSE over iterations
plot(res, type = "l", main = paste("MSE over Iterations (Dropout Rate:", dropout_rate, ")"),
xlab = "Iterations", ylab = "MSE")
# Plot training data and predictions
predictions <- apply(dat[,1, drop = FALSE], 1, yhat, W1, b1, W2, b2)
plot(dat, main = paste("Training Data and Predictions (Dropout Rate:", dropout_rate, ")"))
points(dat[,1], predictions, col = 'red')
# Reset plotting area
par(mfrow = c(1, 1))
}
rate=dropout_rates[1]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
rate=dropout_rates[2]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
rate=dropout_rates[3]
cat("Training with dropout rate:", rate, "\n")
results <- train_neural_network(dropout_rate = rate)
plot_results(results$MSE, rate, results$W1, results$b1, results$W2, results$b2)
#===============================
# (3) K-means algorithm
#===============================
# Load necessary library
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Parameters for the mixture model
n <- 300 # total number of samples
mu1 <- c(2, 2)
mu2 <- c(-3, -1)
mu3 <- c(5, -4)
sigma1 <- matrix(c(1, 0.5, 0.5, 1), ncol=2)
sigma2 <- matrix(c(1.5, -0.5, -0.5, 1.5), ncol=2)
sigma3 <- matrix(c(1, 0, 0, 1), ncol=2)
# Generate samples from each Gaussian distribution
data1 <- mvrnorm(n/3, mu1, sigma1)
data2 <- mvrnorm(n/3, mu2, sigma2)
data3 <- mvrnorm(n/3, mu3, sigma3)
# Combine the data
data <- rbind(data1, data2, data3)
# Convert data to a data frame
data_df <- data.frame(x = data[, 1], y = data[, 2])
# Plot the generated data
ggplot(data_df, aes(x = x, y = y)) +
geom_point(alpha = 0.5) +
ggtitle("Sampled Data from Mixture Model") +
theme_minimal()
#===============================
# (3) K-means algorithm
#===============================
# Load necessary library
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Parameters for the mixture model
n <- 300 # total number of samples
mu1 <- c(2, 2)
mu2 <- c(-3, -1)
mu3 <- c(5, -4)
sigma1 <- matrix(c(1, 0.5, 0.5, 1), ncol=2)
sigma2 <- matrix(c(1.5, -0.5, -0.5, 1.5), ncol=2)
sigma3 <- matrix(c(1, 0, 0, 1), ncol=2)
# Generate samples from each Gaussian distribution
data1 <- mvrnorm(n/3, mu1, sigma1)
#===============================
# (3) K-means algorithm
#===============================
# Load necessary library
library(MASS)  # For mvrnorm function
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Parameters for the mixture model
n <- 300 # total number of samples
mu1 <- c(2, 2)
mu2 <- c(-3, -1)
mu3 <- c(5, -4)
sigma1 <- matrix(c(1, 0.5, 0.5, 1), ncol=2)
sigma2 <- matrix(c(1.5, -0.5, -0.5, 1.5), ncol=2)
sigma3 <- matrix(c(1, 0, 0, 1), ncol=2)
# Generate samples from each Gaussian distribution
data1 <- mvrnorm(n/3, mu1, sigma1)
data2 <- mvrnorm(n/3, mu2, sigma2)
data3 <- mvrnorm(n/3, mu3, sigma3)
# Combine the data
data <- rbind(data1, data2, data3)
# Convert data to a data frame
data_df <- data.frame(x = data[, 1], y = data[, 2])
# Plot the generated data
ggplot(data_df, aes(x = x, y = y)) +
geom_point(alpha = 0.5) +
ggtitle("Sampled Data from Mixture Model") +
theme_minimal()
# Set number of clusters
k <- 3
# Run K-means algorithm
kmeans_result <- kmeans(data_df, centers = k, nstart = 25)
# Add cluster assignment to the data frame
data_df$cluster <- as.factor(kmeans_result$cluster)
# Plot the clusters
ggplot(data_df, aes(x = x, y = y, color = cluster)) +
geom_point(alpha = 0.5) +
ggtitle("K-means Clustering Results") +
theme_minimal()
# Print the cluster centers
print(kmeans_result$centers)
# Print the cluster assignments
table(data_df$cluster)
#===============================
#(2) SVM ksvm
#===============================
# Load necessary libraries
library(kernlab)
library(caret)
# Load the spam dataset from the kernlab package
data(spam)
# Define the training and testing datasets
set.seed(123)  # For reproducibility
trainIndex <- createDataPartition(spam$type, p = .8,
list = FALSE,
times = 1)
spamTrain <- spam[ trainIndex,]
spamTest  <- spam[-trainIndex,]
# Define the C values to be tested
C_values <- c(0.5, 1, 5)
kernel_width <- 0.05
# Prepare to store results
results <- data.frame(C = numeric(), Accuracy = numeric())
# Model training and evaluation
for (C in C_values) {
# Train the SVM model
model <- ksvm(type ~ ., data = spamTrain, kernel = "rbfdot", kpar = list(sigma = kernel_width), C = C)
# Make predictions on the test set
predictions <- predict(model, spamTest)
# Calculate accuracy
accuracy <- sum(predictions == spamTest$type) / nrow(spamTest)
# Store the results
results <- rbind(results, data.frame(C = C, Accuracy = accuracy))
}
# Display the results
print(results)
# Select the best model based on accuracy
best_model_params <- results[which.max(results$Accuracy), ]
best_C <- best_model_params$C
cat("The best C value is:", best_C, "with accuracy:", best_model_params$Accuracy, "\n")
# Train the best model on the entire training data
final_model <- ksvm(type ~ ., data = spam, kernel = "rbfdot", kpar = list(sigma = kernel_width), C = best_C)
# Save the final model (optional)
save(final_model, file = "final_svm_model.RData")
# Print the code of the final model
print(final_model)
#===============================
#START 2023-8-2
#===============================
# KEYWORD:SVM KMEANS
#===============================
#(1) SVM
#===============================
library(kernlab)
# Create training data (x1, x2: predictors, x3: target)
x1 <- sample(0:1,1000,replace = TRUE)
x2 <- sample(0:1,1000,replace = TRUE)
x3 <- as.numeric(xor(x1,x2))
foo <- runif(1000,min = -0.2,max = 0.2)
x1 <- x1 + foo
foo <- runif(1000,min = -0.2,max = 0.2)
x2 <- x2 + foo
# Visualize training data.
plot(cbind(x1,x2),type = "n")
text(cbind(x1,x2),labels = x3)
# Learn SVM with RBF kernel and check training error.
foo <- ksvm(cbind(x1,x2), x3, kernel = "rbfdot", type = 'C-svc')
foo
# Visualize predictions for training data.
prex3 <- predict(foo, cbind(x1,x2))
plot(cbind(x1,x2), type = "n")
text(cbind(x1,x2), labels = prex3)
#===============================
#START 2023-8-2
#===============================
# KEYWORD:SVM KMEANS
#===============================
#(1) SVM
#===============================
#The code below trains a support vector machine (SVM) for classification. The problem consists of two
#continuous inputs, one binary target and 1000 training points. Thus, the problem may seem rather easy.
#However, the SVM does not perform great. Explain why.
library(kernlab)
# Create training data (x1, x2: predictors, x3: target)
x1 <- sample(0:1,1000,replace = TRUE)
x2 <- sample(0:1,1000,replace = TRUE)
x3 <- as.numeric(xor(x1,x2))
foo <- runif(1000,min = -0.2,max = 0.2)
x1 <- x1 + foo
foo <- runif(1000,min = -0.2,max = 0.2)
x2 <- x2 + foo
# Visualize training data.
plot(cbind(x1,x2),type = "n")
text(cbind(x1,x2),labels = x3)
# Learn SVM and check training error.
foo <- ksvm(cbind(x1,x2),x3,kernel = "vanilladot",type = 'C-svc')
foo
# Visualize predictions for training data.
prex3 <- predict(foo,cbind(x1,x2))
plot(cbind(x1,x2),type = "n")
text(cbind(x1,x2),labels = prex3)
library(plotly)
###########################  Code For Assignment 2.8 ###########################
library(shiny)
install.packages("TTR")
library(TTR)
djia = getYahooData("^DJI", start=20060420, end=20160420, freq="daily")
install.packages("quantmod")
install.packages("bnlearn")
library(bnlearn)
data("asia")
bns = list()
for (i in 1:2) {
bns[[i]] = hc(asia, random.graph(colnames(asia)))
plot(bns[[i]])
}
i=1
bns[[i]] = hc(asia, random.graph(colnames(asia)))
plot(bns[[i]])
i=2
bns[[i]] = hc(asia, random.graph(colnames(asia)))
plot(bns[[i]])
all.equal(vstructs(bns[[1]]), vstructs(bns[[2]]))
install.packages("gRain")
library(bnlearn)
library(gRain)
data("asia")
force(asia)
View(asia)
license()
foo <- matrix(data = c(x,y,goal_x,goal_y), nrow = 1)
#install.packages("keras")
library(keras)
# install.packages("ggplot2")
# install.packages("vctrs")
library(ggplot2)
arrows <- c("^", ">", "v", "<")
action_deltas <- list(c(1,0), # up
c(0,1), # right
c(-1,0), # down
c(0,-1)) # left
vis_prob <- function(goal, episodes = 0){
# Visualize an environment with rewards.
# Probabilities for all actions are displayed on the edges of each tile.
# The (greedy) policy for each state is also displayed.
#
# Args:
#   goal: goal coordinates, array with 2 entries.
#   episodes, epsilon, alpha, gamma, beta (optional): for the figure title.
#   H, W (global variables): environment dimensions.
df <- expand.grid(x=1:H,y=1:W)
dist <- array(data = NA, dim = c(H,W,4))
class <- array(data = NA, dim = c(H,W))
for(i in 1:H)
for(j in 1:W){
dist[i,j,] <- DeepPolicy_dist(i,j,goal[1],goal[2])
foo <- which(dist[i,j,]==max(dist[i,j,]))
class[i,j] <- ifelse(length(foo)>1,sample(foo, size = 1),foo)
}
foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),NA,dist[x,y,1]),df$x,df$y)
df$val1 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),NA,dist[x,y,2]),df$x,df$y)
df$val2 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),NA,dist[x,y,3]),df$x,df$y)
df$val3 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),NA,dist[x,y,4]),df$x,df$y)
df$val4 <- as.vector(round(foo, 2))
foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),NA,class[x,y]),df$x,df$y)
df$val5 <- as.vector(arrows[foo])
foo <- mapply(function(x,y) ifelse(all(c(x,y) == goal),"Goal",NA),df$x,df$y)
df$val6 <- as.vector(foo)
print(ggplot(df,aes(x = y,y = x)) +
geom_tile(fill = 'white', colour = 'black') +
scale_fill_manual(values = c('green')) +
geom_tile(aes(fill=val6), show.legend = FALSE, colour = 'black') +
geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
geom_text(aes(label = val5),size = 10,na.rm = TRUE) +
geom_text(aes(label = val6),size = 10,na.rm = TRUE) +
ggtitle(paste("Action probabilities after ",episodes," episodes")) +
theme(plot.title = element_text(hjust = 0.5)) +
scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
}
transition_model <- function(x, y, action, beta){
# Computes the new state after given action is taken. The agent will follow the action
# with probability (1-beta) and slip to the right or left with probability beta/2 each.
#
# Args:
#   x, y: state coordinates.
#   action: which action the agent takes (in {1,2,3,4}).
#   beta: probability of the agent slipping to the side when trying to move.
#   H, W (global variables): environment dimensions.
#
# Returns:
#   The new state after the action has been taken.
delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
final_action <- ((action + delta + 3) %% 4) + 1
foo <- c(x,y) + unlist(action_deltas[final_action])
foo <- pmax(c(1,1),pmin(foo,c(H,W)))
return (foo)
}
DeepPolicy_dist <- function(x, y, goal_x, goal_y){
# Get distribution over actions for state (x,y) and goal (goal_x,goal_y) from the deep policy.
#
# Args:
#   x, y: state coordinates.
#   goal_x, goal_y: goal coordinates.
#   model (global variable): NN encoding the policy.
#
# Returns:
#   A distribution over actions.
foo <- matrix(data = c(x,y,goal_x,goal_y), nrow = 1)
# return (predict_proba(model, x = foo))
return (predict_on_batch(model, x = foo)) # Faster.
}
DeepPolicy <- function(x, y, goal_x, goal_y){
# Get an action for state (x,y) and goal (goal_x,goal_y) from the deep policy.
#
# Args:
#   x, y: state coordinates.
#   goal_x, goal_y: goal coordinates.
#   model (global variable): NN encoding the policy.
#
# Returns:
#   An action, i.e. integer in {1,2,3,4}.
foo <- DeepPolicy_dist(x,y,goal_x,goal_y)
return (sample(1:4, size = 1, prob = foo))
}
DeepPolicy_train <- function(states, actions, goal, gamma){
# Train the policy network on a rolled out trajectory.
#
# Args:
#   states: array of states visited throughout the trajectory.
#   actions: array of actions taken throughout the trajectory.
#   goal: goal coordinates, array with 2 entries.
#   gamma: discount factor.
# Construct batch for training.
inputs <- matrix(data = states, ncol = 2, byrow = TRUE)
inputs <- cbind(inputs,rep(goal[1],nrow(inputs)))
inputs <- cbind(inputs,rep(goal[2],nrow(inputs)))
targets <- array(data = actions, dim = nrow(inputs))
targets <- to_categorical(targets-1, num_classes = 4)
# Sample weights. Reward of 5 for reaching the goal.
weights <- array(data = 5*(gamma^(nrow(inputs)-1)), dim = nrow(inputs))
# Train on batch. Note that this runs a SINGLE gradient update.
train_on_batch(model, x = inputs, y = targets, sample_weight = weights)
}
reinforce_episode <- function(goal, gamma = 0.95, beta = 0){
# Rolls out a trajectory in the environment until the goal is reached.
# Then trains the policy using the collected states, actions and rewards.
#
# Args:
#   goal: goal coordinates, array with 2 entries.
#   gamma (optional): discount factor.
#   beta (optional): probability of slipping in the transition model.
# Randomize starting position.
cur_pos <- goal
while(all(cur_pos == goal))
cur_pos <- c(sample(1:H, size = 1),sample(1:W, size = 1))
states <- NULL
actions <- NULL
steps <- 0 # To avoid getting stuck and/or training on unnecessarily long episodes.
while(steps < 20){
steps <- steps+1
# Follow policy and execute action.
action <- DeepPolicy(cur_pos[1], cur_pos[2], goal[1], goal[2])
new_pos <- transition_model(cur_pos[1], cur_pos[2], action, beta)
# Store states and actions.
states <- c(states,cur_pos)
actions <- c(actions,action)
cur_pos <- new_pos
if(all(new_pos == goal)){
# Train network.
DeepPolicy_train(states,actions,goal,gamma)
break
}
}
}
H <- 4
W <- 4
# Define the neural network (two hidden layers of 32 units each).
model <- keras_model_sequential()
model %>%
layer_dense(units = 32, input_shape = c(4), activation = 'relu') %>%
layer_dense(units = 32, activation = 'relu') %>%
layer_dense(units = 4, activation = 'softmax')
setwd("~/Desktop/GIT/Visualization/Lab/Lab6")
install.packages("tidytext")
setwd("~/Desktop/GIT/Visualization/Lab/Lab6")
library(dplyr)
library(tidyr)
library(plotly)
library(tidytext)
library(wordcloud)
library(visNetwork)
library(RColorBrewer)
library(textdata)
library(shiny)
five_text <- readLines("Five.txt")
textFrame_five_lines <- tibble(text=five_text) %>% mutate(line = row_number())
# concatenate all lines into one string
textFrame_five_single_string <- tibble(text=paste(five_text, collapse=" "))
# one-token-per-row
tidy_frame_five = textFrame_five_lines %>% unnest_tokens(output = word, input = text)
# removing stopwords using anti_join, then count and sort.
# stop_words is a predefined dataset
tidy_frame_five_no_stopword <- tidy_frame_five %>%
anti_join(stop_words, by="word") %>%
count(word, sort=TRUE)
# handle OneTwo.txt
# read text file into string vector
onetwo_text <- readLines("OneTwo.txt")
textFrame_onetwo_lines <- tibble(text=onetwo_text) %>% mutate(line = row_number())
# concatenate all lines into one string
textFrame_onetwo_text_single_string <- tibble(text = paste(onetwo_text, collapse=" "))
# one-token-per-row
tidy_frame_onetwo = textFrame_onetwo_lines %>% unnest_tokens(output = word, input = text)
# removing stopwords, count and sort
tidy_frame_onetwo_no_stopword <- tidy_frame_onetwo %>%
anti_join(stop_words, by="word") %>%
count(word, sort=TRUE)
pal <- brewer.pal(6,"Dark2")
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Wordcloud for the Five-dataset")
tidy_frame_five_no_stopword %>% with(wordcloud(word, n, max.words = 100,
colors=pal, random.order=F))
tidy_frame_onetwo_no_stopword %>% with(wordcloud(word, n, max.words = 100,
colors=pal, random.order=F))
# format the layout of wordcloud, center title etc
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Wordcloud for the OneTwo-dataset")
tidy_frame_onetwo_no_stopword %>% with(wordcloud(word, n, max.words = 100,
colors=pal, random.order=F))
TFIDF <- textFrame_onetwo_lines %>% unnest_tokens(word, text)%>%
mutate(line1=floor(line / 10)) %>%
count(line1,word, sort=TRUE) %>%
bind_tf_idf(word, line1, n)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Wordcloud for the OneTwo-dataset based on TF-IDF(with stop words)")
TFIDF %>%
# internal grouping, output not change if print it, needed for summarise_at
group_by(word) %>%
# calculate mean tf_idf for each word over all documents
summarise(avg_tfidf = mean(tf_idf)) %>%
arrange(desc(avg_tfidf)) %>% # reorder by avg_tfidf to get words we need
with(wordcloud(word, avg_tfidf, max.words = 100, colors = pal, random.order = FALSE))
tidy_frame_sentiment_five <- textFrame_five_lines%>%
unnest_tokens(word, text)%>%
left_join(get_sentiments("afinn"))%>%
mutate(line1=floor(line / 5))%>%
group_by(line1, sort=TRUE)%>%
# change to .groups = "drop" to avoid warning
summarize(Sentiment = sum(value, na.rm = TRUE), .groups = "drop")
tidy_frame_sentiment_five <- textFrame_five_lines%>%
unnest_tokens(word, text)%>%
left_join(get_sentiments("afinn"))%>%
mutate(line1=floor(line / 5))%>%
group_by(line1, sort=TRUE)%>%
# change to .groups = "drop" to avoid warning
summarize(Sentiment = sum(value, na.rm = TRUE), .groups = "drop")
tidy_frame_sentiment_onetwo <- textFrame_onetwo_lines%>%
unnest_tokens(word, text)%>%
left_join(get_sentiments("afinn"))%>%
mutate(line1=floor(line / 5))%>%
group_by(line1, sort=TRUE)%>%
# change from summarize(Sentiment=sum(value, na.rm = T)) to the following one
# to avoid warning
summarize(Sentiment = sum(value, na.rm = TRUE), .groups = "drop")
---
title: "732A98-Visualization Lab 6"
